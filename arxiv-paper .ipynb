{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebb774dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from datetime import date\n",
    "\n",
    "def retrieve_papers_from_arxiv(keywords, start_date, end_date):\n",
    "    \"\"\"\n",
    "    ä»arXivæ£€ç´¢è®ºæ–‡\n",
    "    :param keywords: å…³é”®è¯åˆ—è¡¨\n",
    "    :param start_date: èµ·å§‹æ—¥æœŸ\n",
    "    :param end_date: ç»“æŸæ—¥æœŸ\n",
    "    :return: ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    query = \" OR \".join(keywords)\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=1,  # å¯æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    # print(arxiv.Result.link)\n",
    "    \n",
    "    # results = []\n",
    "    # for result in client.results(search):\n",
    "    #     published = result.published.replace(tzinfo=None)\n",
    "    #     print(published)\n",
    "    #     print(result)\n",
    "    #     if start_date <= published.date() <= end_date:\n",
    "    #         print(published.date())\n",
    "    #         # print(result)\n",
    "    #         results.append(result)\n",
    "\n",
    "    results = []\n",
    "    for result in client.results(search):\n",
    "        published = result.published.replace(tzinfo=None)\n",
    "        if start_date <= published.date() <= end_date:\n",
    "            matched_keywords = [kw for kw in keywords if kw.lower() in (result.title + result.summary).lower()]\n",
    "        \n",
    "            # print(vars(result))\n",
    "            info = {\n",
    "                \"keywords\": matched_keywords,\n",
    "                \"published\": result.published,\n",
    "                \"html_link\": result.entry_id.replace('/abs/', '/html/'),\n",
    "                # result.entry_id,\n",
    "                \"summary\": result.summary,\n",
    "                \"authors\": [author.name for author in result.authors],\n",
    "                \"title\": result.title,\n",
    "                \"article_id\": result.get_short_id() if hasattr(result, \"get_short_id\") else result.entry_id.split('/')[-1]\n",
    "            }\n",
    "            results.append(info)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb77d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "def translate_content(abstracts):\n",
    "    \"\"\"\n",
    "    è¯»å–abstractåˆ—è¡¨ä¸­çš„æ‘˜è¦å†…å®¹\n",
    "    ä½¿ç”¨LLMå°†å†…å®¹ä»è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡\n",
    "    :param content: è¦ç¿»è¯‘çš„è‹±æ–‡å†…å®¹\n",
    "    :return: ç¿»è¯‘åçš„ä¸­æ–‡å†…å®¹\n",
    "    \"\"\"\n",
    "    # llm = ChatOpenAI(\n",
    "    #     api_key=\"sk-bvyamawkigskflwkanovigwdxvlrmajpmnqscmeynxcswwsa\",\n",
    "    #     base_url=\"https://api.siliconflow.cn/v1\",\n",
    "    #     model=\"Qwen/Qwen3-8B\",\n",
    "    #     temperature=0.2,\n",
    "    # )\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        api_key = \"sk-fe47540246614af99efc750075628c17\", \n",
    "        base_url = \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        model_name = \"qwen3-235b-a22b\", \n",
    "        streaming = True,\n",
    "        extra_body={\"enable_thinking\": False},\n",
    "        temperature = 0\n",
    "    )\n",
    "\n",
    "    translated_list = []\n",
    "    for abstract in abstracts:\n",
    "        abstr = abstract[\"summary\"].strip()\n",
    "        id = abstract[\"article_id\"]\n",
    "        messages = [\n",
    "            SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªè‹±è¯‘ä¸­çš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè´Ÿè´£å°†è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ã€‚è¯·ç¡®ä¿ç¿»è¯‘å‡†ç¡®ï¼Œå¹¶ä¿æŒåŸæ–‡çš„è¯­æ°”å’Œé£æ ¼ã€‚\"),\n",
    "            HumanMessage(content=f\"è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼š\\n\\n{abstr}\")\n",
    "        ]\n",
    "        feedback = llm.invoke(messages)\n",
    "        info = {\n",
    "            \"summary_cn\": feedback.content,\n",
    "            \"article_id\": id\n",
    "        }\n",
    "        translated_list.append(info)\n",
    "    return translated_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "def save_to_mysql():\n",
    "    print(\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf134be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def save_to_sqlite(results, translates, db_path=\"arxiv_papers.db\"):\n",
    "    \"\"\"\n",
    "    å°†è®ºæ–‡ä¿¡æ¯å’Œç¿»è¯‘ç»“æœä¿å­˜åˆ°sqliteæ•°æ®åº“\n",
    "    :param results: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨ï¼ˆå­—å…¸ï¼‰\n",
    "    :param translates: ç¿»è¯‘ä¿¡æ¯åˆ—è¡¨ï¼ˆå­—å…¸ï¼Œå«article_idå’Œsummary_cnï¼‰\n",
    "    :param db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„\n",
    "    :return: \"Succeeded!\" æˆ– (\"Failure\", é”™è¯¯ä¿¡æ¯)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        summary_cn_map = {item[\"article_id\"]: item[\"summary_cn\"] for item in translates}\n",
    "\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS papers (\n",
    "                article_id TEXT PRIMARY KEY,\n",
    "                title TEXT,\n",
    "                authors TEXT,\n",
    "                keywords TEXT,\n",
    "                published TEXT,\n",
    "                html_link TEXT,\n",
    "                summary TEXT,\n",
    "                summary_cn TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        for item in results:\n",
    "            article_id = item[\"article_id\"]\n",
    "            title = item[\"title\"]\n",
    "            authors = \", \".join(item[\"authors\"])\n",
    "            keywords = \", \".join(item[\"keywords\"])\n",
    "            published = str(item[\"published\"])\n",
    "            html_link = item[\"html_link\"]\n",
    "            summary = item[\"summary\"]\n",
    "            summary_cn = summary_cn_map.get(article_id, \"\")\n",
    "\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT OR REPLACE INTO papers\n",
    "                (article_id, title, authors, keywords, published, html_link, summary, summary_cn)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (article_id, title, authors, keywords, published, html_link, summary, summary_cn))\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        return \"Succeeded!\"\n",
    "    except Exception as e:\n",
    "        return (\"Failure\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24e7049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'keywords': ['LLM', 'multimodal'], 'published': datetime.datetime(2025, 7, 7, 17, 59, 56, tzinfo=datetime.timezone.utc), 'html_link': 'http://arxiv.org/html/2507.05259v1', 'summary': 'Recent diffusion-based image editing methods have significantly advanced\\ntext-guided tasks but often struggle to interpret complex, indirect\\ninstructions. Moreover, current models frequently suffer from poor identity\\npreservation, unintended edits, or rely heavily on manual masks. To address\\nthese challenges, we introduce X-Planner, a Multimodal Large Language Model\\n(MLLM)-based planning system that effectively bridges user intent with editing\\nmodel capabilities. X-Planner employs chain-of-thought reasoning to\\nsystematically decompose complex instructions into simpler, clear\\nsub-instructions. For each sub-instruction, X-Planner automatically generates\\nprecise edit types and segmentation masks, eliminating manual intervention and\\nensuring localized, identity-preserving edits. Additionally, we propose a novel\\nautomated pipeline for generating large-scale data to train X-Planner which\\nachieves state-of-the-art results on both existing benchmarks and our newly\\nintroduced complex editing benchmark.', 'authors': ['Chun-Hsiao Yeh', 'Yilin Wang', 'Nanxuan Zhao', 'Richard Zhang', 'Yuheng Li', 'Yi Ma', 'Krishna Kumar Singh'], 'title': 'Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing', 'article_id': '2507.05259v1'}]\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "\n",
    "# å…³é”®è¯åˆ—è¡¨\n",
    "keywords = [\n",
    "    \"RAG\", \"agent\", \"LLM\", \"multimodal\", \"embedding\", \"rerank\"\n",
    "]\n",
    "\n",
    "# è®¾ç½®æ—¶é—´æ®µï¼ˆä¾‹å¦‚2024-07-01åˆ°2024-07-06ï¼‰\n",
    "start_date = date(2025, 7, 3)\n",
    "end_date = date(2025, 7, 7)\n",
    "# print(start_date)\n",
    "# print(end_date)\n",
    "\n",
    "results_list = retrieve_papers_from_arxiv(keywords, start_date, end_date)\n",
    "\n",
    "# translates_list = translate_content(results_list)\n",
    "\n",
    "# result = save_to_sqlite(results_list,translates_list)\n",
    "\n",
    "# print(result)\n",
    "print(results_list)\n",
    "# print(translates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a0e567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing Chun-Hsiao Yeh1,3 Yilin Wang3 Nanxuan Zhao3 Richard Zhang3 Yuheng Li2 Yi Ma1,2 Krishna Kumar Singh3 1UC Berkeley 2HKU 3Adobe Abstract Recent diffusion-based image editing methods have significantly advanced text-guided tasks but often struggle to interpret complex, indirect instructions. Moreover, current models frequently suffer from poor identity preservation, unintended edits, or rely heavily on manual masks. To address these challenges, we introduce X-Planner, a Multimodal Large Language Model (MLLM)-based planning system that effectively bridges user intent with editing model capabilities. X-Planner employs chain-of-thought reasoning to systematically decompose complex instructions into simpler, clear sub-instructions. For each sub-instruction, X-Planner automatically generates precise edit types and segmentation masks, eliminating manual intervention and ensuring localized, identity-preserving edits. Additionally, we propose a novel automated pipeline for generating large-scale data to train X-Planner which achieves state-of-the-art results on both existing benchmarks and our newly introduced complex editing benchmark. The project page is available at https://danielchyeh.github.io/x-planner/. Figure 1: Left. Given a source image and complex instruction, our MLLM based X-Planner decomposes the complex instruction into simpler sub-instructions (with edit type) along with auto-generated segmentation masks indicating the editing regions (shown in bottom left of each edited image) and hallucinates additional bounding box of object for the insertion case. We iteratively perform localized editing, by providing X-Plannerâ€™s editing instruction and region (mask and box) to compatible editing model for each edit type. Right. Recent SmartEdit [16] and MGIE [11] which also use MLLM struggles with complex instruction understanding and identity preservation. â€ â€ âˆ—Work done during CHYâ€™s summer internship at Adobe Research. 1 Introduction The field of generative image editing has experienced remarkable advancements in recent years [27, 24, 14, 3, 5, 4], driven by the development of diffusion models [34, 31]. Broadly, these advancements in image editing can be categorized into two categories. The first involves free-form editing, adapts pre-trained diffusion models to edit images based solely on text instructions and a source image [4, 47, 37]. These methods often suffer from over-editing, modifying regions beyond those intended by the user. Subsequent research involve controllable image editing that incorporates user-provided control signalsâ€”such as semantic segmentation masks [27, 3, 50], bounding boxes [6, 21, 42], content dragging and blobs [26, 38, 28], and image prompts [46, 7] to guide task-specific edits. These signals improve editing control but are time-consuming for users to provide manually. Another critical challenge of these models is to robustly interpret and execute complex instructions. Existing methods often struggle with nuanced requirements of these complex instructions, which limits their ability to to perform these edits effectively and provide intuitive, user-friendly interactions to allow more direct user controls over these edits. Below, we identify several key challenges that highlight limitations of current editing models: (1) Multi-Object Targeting Instructions: A single instruction might targets multiple objects within an image which requires editing model to identify each object which needs to be edited according to this single instruction and image content (e.g., first row in Figure 1). (2)Multi-Task Instructions: Instructions with multiple distinct edits within a single prompt (e.g., second row in Figure 1). (3) Indirect Instruction Interpretation: Instructions that contain indirect cues which require deeper understanding and decomposition into multiple steps to achieve accurate results (e.g., last row in Figure 1). Recent diffusion-based image editing methods have significantly advanced text-guided tasks, yet still struggle to robustly interpret and execute complex, indirect instructions. Typically, these tasks require extensive manual effort to simplify instructions and provide precise region guidance, limiting scalability and usability. While MLLMs offer promise due to extensive world knowledge, directly applying them often results in misinterpretation and localization errors (as shown in Figure 1, SmartEdit [16] and MGIE [11] struggle to perform these complex edits as it misunderstand the editing prompt ,e.g., SmartEdit generates ice-cream, instead making it summer scene). One solution is to force these MLLMs to reason about these complex instructions based on image content and apply chain-of-thought reasoning to autonomously break down complex instructions into simpler sub-instructions aligned with image context. In addition, we need these MLLMs to provide region guidance in terms of masks or bounding boxes for these edits. Some existing MLLMs like LISA [20] and GLaMM [32] provide grounding masks while answering image-related questions but have not been trained for localizing editing instructions. Recent work, GenArtist [43], explored this direction by leveraging closed-source GPT-4 [1] for instruction decomposition. However, it has several key limitations: (1) It focuses on breaking down long, nested prompts rather than reasoning about complex and indirect instructions, limiting its effectiveness for intricate editing tasks. (2) GenArtist relies on closed-source model and external toolboxes even during inference, restricting adaptability for users and hindering fine-tuning or further research advancements for the community. (3) Most importantly, its dependence on external object detectors [23] and segmentation model [19] results in bounding boxes and masks that are not optimized for different editing types, leading to failures in tasks requiring more than simple object detection. For instance, in object insertion tasks (e.g., â€add a catâ€) where the cat is not originally present in the image, external detectors and segmentation models (e.g., SAM [19]) struggle to hallucinate and localize the inserted object, failing to provide effective editing guidance. To address these challenges, we propose X-Planner, a Multimodal Large Language Model (MLLM)-driven planning system that excels in managing complex instruction-based image editing tasks as shown in Figure 1. Our X-Planner operates by breaking down intricate user instructions into structured, simpler sub-instructions, each accompanied by auto-generated edit type and mask corresponding to main editing anchor (e.g., in second row of Figure 1,â€œlocal texture: Make &lt;&lt;&lt;tree&gt;&gt;&gt; to be in cyberpunk â€ indicates the edit type to be of texture change and main editing anchor is tree for which we generate mask). This decomposition strategy empowers the model to interpret ambiguous requests and apply stepwise complex edits. Our approach refines spatial control by tailoring masks to each edit typeâ€”tight for color/texture changes, coarse for replacements, and full-image for global editsâ€”preventing over-editing and preserving content outside the mask. In the case of insertion edits, just the mask for editing anchor is not sufficient as the inserted object region can be outside the mask. As shown in Figure 1, the inserted ornaments have to be around the cat which is beyond the mask of catâ€™s body, for which GenArtist [43] by relying on external detector and segmentor would often fail to handle. X-Planner addresses this issue by leveraging the MLLM world knowledge and reasoning about the image content predicts an additional bounding box indicating the region where the object could be inserted. X-Planner also predicts edit type which enables dynamically selecting the most suitable editing model for each edit type. Figure 2: Overview of X-Planner for Complex Instruction-Based Editing. Our X-Planner comprises two main branches: First, MLLM decomposes the complex instruction into multiple simpler sub-instructions along with editing anchors (e.g., cat, dog, and background) which are given to segmentation decoder to get corresponding masks for each sub-instruction. Also, for the insertion edit, MLLM outputs bounding box coordinates along with edit instruction. By integrating with the editing model pool, we then iteratively apply the most suitable editing model to execute each specific edit task based on X-Planner generated sub-instruction along with masks / bounding boxes. In order to train such planner, we need a large-scale training data mapping complex instruction to simple instructions with edit type, segmentation masks, and bounding boxes for insertion edits. Currently, no such dataset exists, hence we created large-scale Complex Instruction-based Editing Dataset (COMPIE). It comprises of over 260K paired complex-simple instructions along with mask and bounding box annotations for insertion edit. COMPIE is designed with our novel automated data annotation pipeline and stringent quality verification processes, ensuring the dataset is both scalable and highly reliable for evaluating editing capabilities. In light of the lack of benchmarks for evaluating complex instruction-driven image editing, we also propose a comprehensive evaluation protocol and a benchmark focusing on complex instructions. To sum up, our contributions can be summarized as follows: â€¢ X-Planner: Novel end-to-end, self-contained MLLM-based planner to support complex image editing. We introduce X-Planner, an MLLM-driven agent that automatically decomposes complex user instructions into simpler tasks, with auto-generated masks and boxes for insertion edit task without relying on external MLLMs and detectors or segmentors during test time. â€¢ A fully automated pipeline for creating large-scale training dataset for complex editing planning. To support the training of X-Planner, we present an automatic large-scale dataset creation pipeline for generating complex-simple instruction pairs, segmentation masks, bounding boxes, and edit types. â€¢ New complex instruction-based editing benchmark. We introduce a large-scaled curated benchmark, COMPIE that targets compositional and indirect instructions requiring world knowledge that aims to catalyze on real-world editing tasks beyond existing editing benchmarks. 2 Related Works Controllable Generative Image Editing. Text-to-image diffusion models have recently demonstrated remarkable performance in generating high-quality images from textual descriptions [34, 31]. Building on this success, pre-trained diffusion models have been adapted for image editing tasks guided by editing text input. Some of these are training free methods [25, 14, 29, 5, 15, 44] like SD-Edit [25], Prompt-to-Prompt [14], and Pix2PixZero [29] which perform text-based image editing by injecting noise into the image and then guiding the diffusion process to align with the editing text. Another line of works are training based methods [4, 47, 11, 12] like InstructPix2Pix [4], MagicBrush [47] which are more robust than training-free methods and relies on creating paired data of original and edited image to fine-tune the text-to-image diffusion models. While these methods improve upon earlier approaches, they often suffer from over-editing, affecting regions unrelated to the userâ€™s instructions. Subsequent methods [27, 3, 50, 6, 21, 42, 26, 38, 28, 46, 7] have improved controllable image editing by incorporating additional control signals, such as segmentation masks, bounding boxes, dragging, blobs, and image prompts. However, these methods often require users to provide control guidance manually, which can be laborious and limits usability. Also, they are generally limited to simple, direct instructions and struggle with more complex ones. Our X-Planner overcomes these challenges by automatically decomposing user instructions into actionable sub-instructions, generating the segmentation masks and bounding boxes as control guidance. MLLM-based Image Editing. Multimodal Large Language Models (MLLMs) leverage the strengths of the large language models (LLMs) [9, 40, 10] while incorporating visual data [22, 52], enabling more sophisticated multimodal understanding and generation. Recent efforts have extended MLLMs to the domain of image editing [16, 11]. E.g., MGIE [11] uses MLLMs to make editing instructions more expressive and use that to guide editing models such as InstructPix2Pix to have better image editing ability. Also, a very recent concurrent work, GenArtist [43] uses an external closed-source GPT-4 [1] as agent that decomposes long editing tasks with multiple simple instructions nested together (similar to multi-task instructions mentioned in the introduction) and uses external object localization tools. In contrast, our proposed X-Planner is an MLLM agent which handles actual indirect complex instructions and generates editing type specific masks which are beyond simple object detection (e.g. for insertion it hallucinates object location based on image contnet). Moreover, X-Planner operates independently at inference, without relying on large, closed-source models like GPT-4 for planning. 3 Method In this section, we introduce X-Planner (Figure 2), a method specifically designed to break down complex instructions into simpler image editing tasks. Leveraging an MLLM trained on our proposed large-scale dataset tailored for instruction decomposition, X-Planner autonomously generates control inputsâ€”such as segmentation masks and bounding boxesâ€”for each sub-instruction to facilitate precise and instruction-based image editing. 3.1 X-Planner: A Complex Editing Task Agent We first break down this task of complex-instruction based image editing planning into key sub-problems of (1) complex instruction decomposition, and (2) control guidance input generation. Figure 2 presents an overview of our X-Planner pipeline, which first decompose complex instructions into multiple simpler instructions along with corresponding control guidance inputs (mask for all edits and bounding box for insertion edit). We then conduct the iterative editing by assigning the suitable editing model for each edit task. We draw inspiration from the recently introduced GLaMM [32], which is an MLLM that utilizes an LLaVA-like architecture [22] with a segmentation mask decoder. This model constructs image-level captions with specific phrases linked to corresponding segmentation masks. For example, given an image with a cat and a dog, GLaMM can respond to â€˜can you describe this image?â€™ with â€˜there are a &lt;&lt;&lt;cat&gt;&gt;&gt; and a &lt;&lt;&lt;dog&gt;&gt;&gt;â€™, anchored to unique segmentation masks for each animal. So, GLaMM would use vision-language understanding of MLLM to generate the caption along with anchored phrases (like cat and dog) and then segmentation mask decoder takes the input image feature and these anchored phrases to segment them. In order to adapt the GLaMM for our case, we would like it to take a source image and a complex instruction, break it down to simpler instructions with edit type and anchored editing object/region (e.g. cat, dog, and background in Figure 2). Thus, segmentation decoder can take these anchored editing regions to get corresponding masks. For the insertion case, we also want to predict the location of the object to be inserted but we cannot use the segmentation decoder as it can only segment the object visible in the image. Hence, we would use MLLM which has world-knowledge to predict bounding box based on the input image and insertion instruction (e.g. [insertion]&lt;&lt;&lt; 0.59,0.71,0.95,0.93&gt;&gt;&gt; Add Christmas ornaments around the cat in Figure 2). However, we realize that GLaMM does not work well for complex instruction-based planning tasks. Specifically, (1) it struggles with complex instruction decomposition, as it primarily learns from visual grounding samples rather than the instruction interpretation, and (2) it is limited in generating control guidance inputs, such as task-specific masks, especially for insertion tasks where it fails to hallucinate unseen objects. These two main problems remain unsolved in the current GLaMM, thus motivating the need for a large-scale, complex image editing instruction planning dataset to train MLLM and segmentation decoder of GLaMM model to generalize for this specific task. Figure 3: Level 1: Complex-Simple Instruction Pair Generation. Using our structured template, we prompt GPT-4o to generate complex instructionsâ€”including indirect, multi-object, and multi-task instructions (as defined in Section 1)â€”along with their corresponding simpler instructions, object anchors, and edit types. 3.2 Automated Data Annotation Pipeline We present our novel automated annotation pipeline developed to construct the Complex Instruction-Based Editing Dataset (COMPIE), a comprehensive and diverse dataset for complex instruction-driven editing planner. The pipeline comprises three distinct levels. At Level 1 (Figure 3), we generate structured complex and decomposed simple instruction (with editing anchors) pairs using GPT-4o [1] for an input image. Level 2 (Figure 4) employs Grounded-SAM [33] to produce initial segmentation masks anchored to each instruction, providing a base for spatial control, and these masks are further refined according to edit type. Level 3 (Figure 5) focuses on insertion-based instructions to generate precise bounding boxes indicating location where object can be inserted. Level 1: Complex-Simple Instruction Pair Generation. To address the challenge of limited instruction diversity, we generate complex editing instructions by leveraging MLLM creativity and human oversight. We draw on diverse data sourcesâ€”including SEED-X [12], UltraEdit [50], and InstructPix2Pix [4] datasetsâ€”spanning synthetic to real images and varying quality levels. First, we manually design in-context examples that capture key instruction types: indirect, multi-object, and multi-task instructions (as defined in Section 1). We ensure that each complex instruction decomposes into 1 to 5 simpler sub-instructions with meaningful and coherent correspondence, and that each sub-instruction specifies an editing task type and editing anchor which corresponds to edited object/region. Each simple edit will have one anchor except replace edit which will have two anchors corresponding to objects before and after replace edit. Using GPT-4o [1], we then generate four complex-simple instruction pairs per image by prompting it with both the source image and our designed task template (see Figure 3). Also, in our training data we mix simple-simple instruction pairs to ensure X-Planner learns to not modify or breakdown simple instructions. Our approach also works using the open-sourced models like Pixtral-Large [2]. Please see Supp. for more details. Figure 4: Level 2: Instruction-Based Mask Generation and Refinement. In Stage 1, we use the source image and anchor text with Grounded SAM to generate a fine-grained mask for the specified object. In Stage 2, we refine this mask by applying varies strategies based on the edit type provided in Level 1 (Figure 3). Figure 5: Level 3: Insertion Task-Based Mask &amp; Box Localization. For insertion task, Grounded SAM struggles to segment objects not present in the source image. We pre-train an MLLM on a bounding box-annotated dataset [41], enabling it to pseudo-annotate our data with bounding box for insertion edits. Level 2: Instruction Mask Generation and Refinement. X-Planner aims to provide editing models with precisely defined target regions for modification. At Level 2 (Stage 1), we generate segmentation masks based on the edited object anchor identified in each decomposed instruction from Level 1 (e.g., â€œadd a circus ring behind the lion; Anchor: &lt;&lt;&lt;lion&gt;&gt;&gt;â€). Using the source image and anchor text, we employ Grounded SAM [33] to produce a fine-grained mask for the specified object. In Stage 2, we refine the mask based on the edit type (see Figure 3). For local texture, color change, and background edits, we use the mask generated in Stage 1 directly. For shape changes, we make masks larger to accommodate object reshaping by dilating the mask by 20%. In replace tasks (e.g., â€œreplace cat with dogâ€), we take the union of both masks (e.g. cat and dog) based on the anchor objects from pre- and post-edit images if available (e.g., InstructPix2Pix dataset). For datasets like SEED-X, where only the pre-edit image is available, we dilate the mask by 20% to account for the replace edit. For style changes or instructions requiring global transformation (e.g., â€œmake image 1950â€™s styleâ€), we select the entire image as the editing mask. Level 3: Insertion Task-Based Box Localization. In insertion-type edits (e.g., â€œadd a circus ring behind the lionâ€ as shown in Figure 3), Grounded SAM, which relies on object anchors, encounters difficulties when segmenting objects that are not present in the source image (e.g., â€œthe circus ringâ€). Direct segmentation of the intended placement region (e.g., â€œlionâ€) often leads to imprecise masks, particularly when there is ambiguity between the instruction and the mask (e.g., â€œadd a circus ring behind the lionâ€), causing errors by segmenting the existing object (lion) rather than the intended insertion area (behind the lion). This discrepancy can lead to degraded edit quality. To address these limitations, in Level 3 (Stage 2), we fine-tune the MLLM component of GLaMM [32] on an annotated dataset with ground truth (GT) bounding boxes for insertion locations. Specifically, we leverage the MULAN dataset [41], which includes background images with and without foreground objects, allowing us to use images without the object as input and predict the bounding box of the insertion target. Through training, the MLLM learns to generate bounding box recommendations for novel insertion instructions during inference. For unannotated insertion instructions, the fine-tuned MLLM produces pseudo-labels by predicting bounding boxes based on the given instruction, enabling precise edit placements. In our experiments (Section 4.3), we further demonstrate X-Plannerâ€™s capability to generate consistent bounding box predictions with repeated instruction to show plausible variations in location. Figure 6: Qualitative Comparison for Complex Instruction-Based Editing Benchmark. Integrating X-Planner with editing methods, InstructPix2Pix* and UltraEdit, brings drastic boosts in preserving object identities with X-Planner generated masks and boxes (display in bottom-left of each image). X-Plannerâ€™s decomposition of complex instructions also enhances alignment with various complex instruction inputs. X-Planner provides a distinct advantage over baselines that only use the source image and complex instruction without masks. 4 Experiments Our experiments evaluate the quality of X-Planner in complex instruction understanding and editing localization. First, we test performance on simple instruction settings using the established MagicBrush benchmark [47]. Second, we evaluate complex instruction settings by comparing performance with and without plugging in X-Planner to baselines on our proposed COMPIE benchmark. Settings. X-Planner uses GLaMM [32] as base model, built on Vicuna-7B [51]. For training, over 260K complex-simple instruction pairs COMPIE is used. See supp. for more details. Baselines. For the MagicBrush benchmark, which focuses on simple instruction settings, we benchmark against methods tailored for straightforward edits [4, 48, 47, 25, 27, 3, 50]. We explore variations that integrate components of X-Planner into the UltraEdit [50] baseline, which is current state-of-the-art and can also take editing mask as input to assess the impact of X-Planner in simpler editing scenarios. For the COMPIE benchmark, which addresses complex instruction settings, we select UltraEdit [50] and InstructPix2Pix* as primary baselines to show the effectiveness of our X-Planner. InstructPix2Pix* is an improved version of InstructPix2Pix [4] using our internal dataset and also utilizes mask as input conditioning. We use this model to show generalizability of our X-Planner due to lack of public models with mask conditioning. We evaluate these methods in two variants: with X-Planner integration and without it. Also, we compare with MGIE [11] and SmartEdit [16], which use MLLM to improve the editing performance. Benchmark and Metrics. For the MagicBrush benchmark [47], we use its evaluation setup, and for our edits measure L1, L2 distance, CLIP-I, and DINO similarity with ground-truth. For the COMPIE benchmark, we adopt the evaluation protocol from EmuEdit [37], comparing edited images against both the source image and target captions. Consistent with EmuEdit, we use L1, CLIP image similarity (Câ¢Lâ¢Iâ¢Piâ¢mğ¶ğ¿ğ¼subscriptğ‘ƒğ‘–ğ‘šCLIP_{im}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_i italic_m end_POSTSUBSCRIPT), and DINO similarity to measure how well the edited image retains the content of the original image. And use CLIP text-image similarity (Câ¢Lâ¢Iâ¢Poâ¢uâ¢tğ¶ğ¿ğ¼subscriptğ‘ƒğ‘œğ‘¢ğ‘¡CLIP_{out}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT) to measure alignment of editing instruction and edited image. Given that Câ¢Lâ¢Iâ¢Poâ¢uâ¢tğ¶ğ¿ğ¼subscriptğ‘ƒğ‘œğ‘¢ğ‘¡CLIP_{out}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT can struggle to capture the nuances of complex instructions, we additionally employ InternVL2-Llama3-76B [8], a powerful MLLM, to evaluate the alignment between the editing instruction and edited image (Mâ¢Lâ¢Lâ¢Mtâ¢iğ‘€ğ¿ğ¿subscriptğ‘€ğ‘¡ğ‘–MLLM_{ti}italic_M italic_L italic_L italic_M start_POSTSUBSCRIPT italic_t italic_i end_POSTSUBSCRIPT). We use InternVL2 for fair evaluation instead of GPT-4o as GPT-4o was used to create our training data. For completeness, we also use InternVL2 to measure the similarity of input image and edited image (Mâ¢Lâ¢Lâ¢Miâ¢mğ‘€ğ¿ğ¿subscriptğ‘€ğ‘–ğ‘šMLLM_{im}italic_M italic_L italic_L italic_M start_POSTSUBSCRIPT italic_i italic_m end_POSTSUBSCRIPT). Please find Supp. for more details. Table 1: Quantitative Comparison on the MagicBrush Test Set. We report results for both single-turn and multi-turn settings. In comparison to UltraEdit using human labeled masks, we evaluate using X-Planner generated masks and bounding boxes as control inputs. For Bag of Models, we utilize PowerPaint for removal tasks, InstructDiff for style changes, and UltraEdit for other edit types. Single-Turn Methods Guidance Control Input L1â†“â†“\\downarrowâ†“ L2â†“â†“\\downarrowâ†“ CLIP-Iâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ SD-SDEdit No 0.1014 0.0278 0.8526 0.7726 Null Text Inversion No 0.0749 0.0197 0.8827 0.8206 GLIDE Human Labeled Mask 3.4973 115.8347 0.9487 0.9206 Blended Diffusion Human Labeled Mask 3.5631 119.2813 0.9291 0.8644 HIVE No 0.1092 0.0380 0.8519 0.7500 InstructPix2Pix (IP2P) No 0.1141 0.0371 0.8512 0.7437 IP2P w/ MagicBrush No 0.0625 0.0203 0.9332 0.8987 UltraEdit No 0.0614 0.0181 0.9197 0.8804 UltraEdit Human Labeled Mask 0.0575 0.0172 0.9307 0.8982 X-Planner + UltraEdit X-Plannerâ€™s Mask 0.0528 0.0171 0.9281 0.8900 X-Planner + UltraEdit X-Plannerâ€™s Mask + Box 0.0513 0.0168 0.9312 0.8959 X-Planner + Bag of Models X-Plannerâ€™s Mask + Box 0.0511 0.0172 0.9331 0.8970 Multi-Turn Methods Guidance Control Input L1â†“â†“\\downarrowâ†“ L2â†“â†“\\downarrowâ†“ CLIP-Iâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ SD-SDEdit No 0.1616 0.0602 0.7933 0.6212 Null Text Inversion No 0.1057 0.0335 0.8468 0.7529 GLIDE Human Labeled Mask 11.7487 1079.5997 0.9094 0.8494 Blended Diffusion Human Labeled Mask 14.5439 1510.2271 0.8782 0.7690 HIVE No 0.1521 0.0557 0.8004 0.6463 InstructPix2Pix (IP2P) No 0.1345 0.0460 0.8304 0.7018 IP2P w/ MagicBrush No 0.0964 0.0353 0.8924 0.8273 UltraEdit, eval w/o region No 0.0780 0.0246 0.8954 0.8322 UltraEdit, eval w/ region Human Labeled Mask 0.0745 0.0236 0.9045 0.8505 X-Planner + UltraEdit X-Plannerâ€™s Mask 0.0679 0.0227 0.9025 0.8423 X-Planner + UltraEdit X-Plannerâ€™s Mask + Box 0.0668 0.0226 0.9047 0.8475 X-Planner + Bag of Models X-Plannerâ€™s Mask + Box 0.0665 0.0223 0.9079 0.8508 4.1 MagicBrush Results (Simple Instructions) Table 1 shows quantitative results on the MagicBrush benchmark. Key observations: (1) X-Planner enhances UltraEdit by providing masks and bounding boxes for localized edits, improving performance, especially for insertion tasks. X-Plannerâ€™s mask is able to match the human labeled mask, and even outperform it sometimes as shown in the result. (2) X-Planner is model-agnostic, integrating seamlessly with multiple models (e.g., PowerPaint [53] for removal, InstructDiff [13] for style changes, and UltraEdit for other edits), leveraging their strengths to boost overall performance. Please find Supp. for more quantitative results. 4.2 COMPIE-Eval Results (Complex Instructions) Qualitative Results. In Figure 6, we show complex instruction editing results for InstructPix2Pix* and UltraEdit with and without X-Planner. We can see that without X-Planner editing methods are not able to understand the user intentions from complex instructions. For example, in the last row of Figure 6, the intention of the instruction was to make image look futuristic but just UltraEdit fails to understand that whereas our X-Planner is able to convert complex instruction into meaningful sub-instructions. Even for the cases where editing method understands the meaning of instructions, they struggle with identity preservation as they cannot leverage the editing masks and bounding boxes of the planner (e.g. third row for InstructPix2Pix* in Figure 6). Table 2: Quantitative Comparison on the COMPIE Benchmark. X-Planner significantly improves the editing performance of UltraEdit and InstructPix2Pix* by decomposing complex instructions and providing control guidance inputs (e.g., masks). To overcome the limitations of Câ¢Lâ¢Iâ¢Poâ¢uâ¢tğ¶ğ¿ğ¼subscriptğ‘ƒğ‘œğ‘¢ğ‘¡CLIP_{out}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT in handling complex instructions, we utilize an MLLM-based evaluation metric to better reflect X-Plannerâ€™s capabilities. Methods Guidance Control Input L1â†“â†“\\downarrowâ†“ CLIPim â†‘â†‘\\uparrowâ†‘ CLIPoutâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ MLLMti â†‘â†‘\\uparrowâ†‘ MLLMim â†‘â†‘\\uparrowâ†‘ SmartEdit [16] No 0.2764 0.7713 0.2512 0.6044 0.6511 0.5347 MGIE [11] No 0.2988 0.7692 0.2498 0.5981 0.6408 0.5288 UltraEdit No 0.1292 0.7688 0.2698 0.6387 0.6652 0.5523 GenArtist [43] + UltraEdit GenArtistâ€™s Mask + Decomposed Instruction 0.1253 0.7767 0.2621 0.6435 0.6894 0.5593 X-Planner + UltraEdit X-Plannerâ€™s Decomposed Instruction 0.1253 0.7767 0.2621 0.6435 0.6894 0.5593 X-Planner + UltraEdit X-Plannerâ€™s Mask + Decomposed Instruction 0.1188 0.7875 0.2569 0.6599 0.7061 0.5744 InstructPix2Pix* No 0.1517 0.8020 0.2666 0.6988 0.6727 0.6160 GenArtist [43] + InstructPix2Pix* GenArtistâ€™s Mask + Decomposed Instruction 0.1458 0.8143 0.2641 0.7114 0.7072 0.6277 X-Planner + InstructPix2Pix* X-Plannerâ€™s Decomposed Instruction 0.1458 0.8143 0.2641 0.7114 0.7072 0.6277 X-Planner + InstructPix2Pix* X-Plannerâ€™s Mask + Decomposed Instruction 0.1320 0.8285 0.2591 0.7068 0.7408 0.6454 Quantitative Results. We create a high quality and diverse test benchmark, COMPIE-Eval, focusing on complex editing by collecting data from a different sources, LAION-high-aesthetics [35], and Unsplash-2K [18]. We then use GPT-4o to generate complex instruction for the test image and apply a post-verification stage, in which crowd workers filter examples with irrelevant instructions. The COMPIE contains 550 images, with complex instructions with variations mentioned in the introduction. See supp. for details. In Table 2, we show that X-Planner enhances editing performance by simplifying complex instruction to decomposed simple instructions suitable for editing models and generating masks helps it further for improved identity preservation. It improves UltraEdit and InstructPix2Pix* across most metrics, except Câ¢Lâ¢Iâ¢Poâ¢uâ¢tğ¶ğ¿ğ¼subscriptğ‘ƒğ‘œğ‘¢ğ‘¡CLIP_{out}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT, which struggles with edited image and complex instruction alignment (e.g., focusing on ice cream instead of the summer scene in Figure 1 last row). To address this, we use InternVL2, an MLLM-based text-image alignment metric (Mâ¢Lâ¢Lâ¢Mtâ¢iğ‘€ğ¿ğ¿subscriptğ‘€ğ‘¡ğ‘–MLLM_{ti}italic_M italic_L italic_L italic_M start_POSTSUBSCRIPT italic_t italic_i end_POSTSUBSCRIPT), which better understands complex instructions, showing significant improvements with X-Planner. MGIE [11] and SmartEdit [16] despite using MLLM gives inferior results. Please note that InternVL2 can also be used for verifying the quality of the edited results and choose the best one. Figure 7: User Study on COMPIE Benchmark. We compare against InstructPix2Pix* and UltraEdit. â€œBetterâ€ means the generated images by using our X-Planner is preferred and vice versa. Table 3: Segmentation Mask Comparison on PIE Benchmark. We compare on the setting of instruction-to-segmentation mask, X-Planner consistently outperforms baseline methods. Method IoUâ†‘â†‘\\uparrowâ†‘ Precisionâ†‘â†‘\\uparrowâ†‘ Recallâ†‘â†‘\\uparrowâ†‘ Random 10% Mask 0.09 0.49 0.12 GLaMM-Base 0.14 0.66 0.15 GLaMM-RefSeg 0.28 0.69 0.32 Llama3+GLaMM 0.44 0.73 0.53 X-Planner (Ours) 0.67 0.79 0.81 User Study. We conducted a user study with 100 random samples from 550 images in the COMPIE benchmark to compare results of two baselines: InstructPix2Pix* and UltraEdit with and without X-Planner. Participants rated each image on (1) identity preservation, (2) instruction alignment, and (3) overall quality, and choose the preferred image or rate them equal. In Figure 7, we show average results for both benchmarks and observe the users prefer results with X-Planner for all criteria (better means X-Planner preferred). Figure 8: Visualize Consistent Bounding Box with Repeated Runs. We show X-Planner can generate consistent bounding boxes with repeated runs to yield plausible location variations. 4.3 GLaMM Comparison and BBox Localization In Table 3, we report X-Plannerâ€™s segmentation mask generation performance on the PIE benchmark [17], comparing it with GLaMM [32] variations (baseline model and a version fine-tuned on RefSeg dataset to have better grounding) and a baseline that leverages Llama3 [10] for object anchoring followed by GLaMM for mask generation. Based on the results, we can see X-Planner consistently surpasses other instruction-to-segmentation methods, underscoring its effectiveness for mask generation in instruction-based tasks. We visualize X-Plannerâ€™s bounding box localization for insertion edits in Figure 8 with 10 repeated runs of the same instruction. Predicted box locations (laptop on table) and shapes (vertical for palm tree) appear plausible. Detailed quantitative analysis and ablations are in the Supp. 5 Conclusion In this paper, we introduced X-Planner, an MLLM-based planning system that breaks down complex instructions into simpler tasks with editing masks and bounding boxes for insertion edits. We also proposed a novel data generation pipeline to train this planner. Our evaluation highlights X-Plannerâ€™s potential to enhance existing editing models, encouraging further exploration of MLLM-based planners as complementary tools for complex editing tasks. References Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Agrawal et al. [2024] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. Avrahami et al. [2022] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18208â€“18218, 2022. Brooks et al. [2023] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392â€“18402, 2023. Cao et al. [2023] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22560â€“22570, 2023. Chen et al. [2024a] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 5343â€“5353, 2024a. Chen et al. [2024b] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6593â€“6602, 2024b. Chen et al. [2024c] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. Internvl2: Better than the bestâ€”expanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024c. Chowdhery et al. [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1â€“113, 2023. Dubey et al. [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu et al. [2023] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. arXiv preprint arXiv:2309.17102, 2023. Ge et al. [2024] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Geng et al. [2024] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: A generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12709â€“12720, 2024. Hertz et al. [2022] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Hertz et al. [2024] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4775â€“4785, 2024. Huang et al. [2024] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362â€“8371, 2024. Ju et al. [2024] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In The Twelfth International Conference on Learning Representations, 2024. Kim and Son [2021] Younggeun Kim and Donghee Son. Noise conditional flow model for learning the super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 2021. Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4015â€“4026, 2023. Lai et al. [2024] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9579â€“9589, 2024. Li et al. [2023] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511â€“22521, 2023. Liu et al. [2024a] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Liu et al. [2024b] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 38â€“55. Springer, 2024b. Lugmayr et al. [2022] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11461â€“11471, 2022. Meng et al. [2021] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Mou et al. [2023] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. Nichol et al. [2021] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Nie et al. [2024] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. arXiv preprint arXiv:2405.08246, 2024. Parmar et al. [2023] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 1â€“11, 2023. Peng et al. [2024] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: A human-aligned benchmark for personalized image generation. arXiv preprint arXiv:2406.16855, 2024. Podell et al. [2023] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Rasheed et al. [2024] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13009â€“13018, 2024. Ren et al. [2024] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684â€“10695, 2022. Schuhmann et al. [2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278â€“25294, 2022. Shen et al. [2024] Tiancheng Shen, Jun Hao Liew, Long Mai, Lu Qi, Jiashi Feng, and Jiaya Jia. Empowering visual creativity: A vision-language assistant to image editing recommendations. arXiv preprint arXiv:2406.00121, 2024. Sheynin et al. [2024] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871â€“8879, 2024. Shi et al. [2024] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8839â€“8849, 2024. Team et al. [2024] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Tudosiu et al. [2024] Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, and Sarah Parisot. Mulan: A multi layer annotated dataset for controllable text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22413â€“22422, 2024. Wang et al. [2024a] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6232â€“6242, 2024a. Wang et al. [2024b] Zhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for unified image generation and editing. arXiv preprint arXiv:2407.05600, 2024b. Wu et al. [2024] Zongze Wu, Nicholas Kolkin, Jonathan Brandt, Richard Zhang, and Eli Shechtman. Turboedit: Instant text-based image editing. ECCV, 2024. Xiao et al. [2024] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. Ye et al. [2023] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. Zhang et al. [2024a] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: A manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024a. Zhang et al. [2024b] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9026â€“9036, 2024b. Zhang et al. [2025] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. Zhao et al. [2024] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024. Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595â€“46623, 2023. Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. Zhuang et al. [2023] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. A task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023. Overview In this supplementary, we first provide additional details about our training dataset and proposed evaluation benchmark in Section 6 and X-Plannerâ€™s implementation details in Section 7. Next, we show some more quantitative results for X-Plannerâ€™s bounding box guidance ability in Section 8. Then, We demonstrate our planner trained with open-sourced model and show various qualitative comparisons with baseline methods to show the effectiveness of our planner in Section 9, 10, 11, 12, 13, and 14. Figure 9: X-Plannerâ€™s Training Dataset Summary (Generated from GPT-4o). This figure provides a distribution summary of our X-Plannerâ€™s training dataset, including (a) the data sources, (b) the distribution of edit types for decomposed instructions, and (c) the number of decomposed instructions. The dataset demonstrates significant diversity and comprises large-scaled number of pairs. Note that nğ‘›nitalic_n in (a) indicates the number of data samples. Figure 10: COMPIE Benchmark Summary. This figure summarizes the proposed COMPIE benchmark, which consists of 550 samples spanning various types of complex instructions shown in (a), including (1) general complex instructions, (2) indirect instructions, (3) multi-object instructions, and (4) multi-task instructions. Additionally in (b), we present the word count distribution of complex instruction anchor descriptions, highlighting the diversity of the dataset. Note that nğ‘›nitalic_n in (a) indicates the number of data samples. 6 COMPIE Dataset &amp; Benchmark Summary X-Plannerâ€™s Training Dataset Statistics (COMPIE). We explore the details of our proposed COMPIE, a large-scale and high-quality dataset specifically designed to address complex instruction-based image editing. COMPIE contains over 260K complex-to-simple instruction pairs, focusing on complex editing tasks unlike previous works (e.g., MagicBrush) that predominantly focus on simple instructions. Figure 9 provides a comprehensive summary of the dataset, including (a) data sources such as SEEDX [12], UltraEdit [50], MULAN [41], and InstructPix2Pix [4], (b) the distribution of edit types across 8 categories for decomposed instructions, and (c) the breakdown of decomposed instructions ranging from 1 to 5 per complex instruction. COMPIE Validation Benchmark Statistics. In this section, we present a detailed summary of the COMPIE validation benchmark, highlighting its diversity and focus on complex instruction-based image editing. COMPIE benchmark comprises 550 samples from LAION high-aesthetic dataset [35] and Unsplash 2K [18], providing a rich variety of real-world images to enhance generalization across different domains. The COMPIE benchmark is categorized into four distinct types of complex instructions, including (1) general complex instructions (50%). General complex instructions typically require multiple editing steps to fulfill the directive. For instance, instructions such as â€˜Make the image look like a winter wonderlandâ€™ or â€˜Transform the room to appear colorful and livelyâ€™ necessitate edits across multiple regions or objects to achieve the desired outcome comprehensively, (2) indirect instructions (30%), (3) multi-object instructions (15%), and (4) multi-task instructions (5%), as shown in Figure 10 (a). This diverse distribution ensures comprehensive coverage of various complexities encountered in real-world editing tasks. Figure 10 (b) illustrates the word count distribution of complex instruction anchor descriptions, underscoring the instruction diversity in the dataset. The inclusion of diverse anchor words enriches the datasetâ€™s ability to evaluate the interpretation ability of editing models. These insights showcase COMPIE as a robust benchmark designed to advance the development of image editing models which understand and execute complex instructions. 7 Implementation Details X-Plannerâ€™s Setup. Our X-Planner leverages GLaMM [32] as the base model, built on Vicuna-7B [51]. The approach incorporates key components inspired by GLaMM [32], particularly the design of the region encoder, grounding image encoder, and pixel decoder. Adhering to the training protocol of GLaMM, we keep the global image encoder and grounding image encoder frozen, while fully fine-tuning the region encoder and pixel decoder. For the language model, we employ LoRA fine-tuning with a scaling factor Î±=8ğ›¼8\\alpha=8italic_Î± = 8 over 10 epochs. MLLM Evaluation Metric Setup. In the COMPIE benchmark evaluation, we follow the Emu Edit [37] protocol for metrics and employ InternVL2-Llama3-76B [8] as our Mâ¢Lâ¢Lâ¢Mğ‘€ğ¿ğ¿ğ‘€MLLMitalic_M italic_L italic_L italic_M metric to evaluate instruction-to-edited-image alignment and similarity between the original and edited images. Specifically, we adapt the DreamBench++ [30] template for image-to-image alignment by ranking alignment scores from very poor to excellent (0 to 4) based on shape, color, and texture criteria. For instruction-to-image alignment, we ensure that text prompts are complex instructions requiring transformation. For example, the prompt â€˜Could you make this image look like the season when ice cream is a daily essential?â€™ is first interpreted as creating a summer scene rather than directly associating the prompt with ice cream. We normalize the maximum score to 4 and calculate the performance percentage based on the average score, presenting it as the Mâ¢Lâ¢Lâ¢Mğ‘€ğ¿ğ¿ğ‘€MLLMitalic_M italic_L italic_L italic_M metric performance. X-Plannerâ€™s Training Dataset Setup. For training dataset distribution, we ensure that X-Planner retains the ability to handle simple instructions. In the InstructPix2Pix [4] dataset, 40% of training pairs are simple-to-simple, meaning the decomposition directly maps simple instructions to simple outputs. Similarly, for the MULAN [41] dataset, which focuses on insertion-type edits, we treat the dataset pairs as simple-to-simple for insertion-specific training. For fine-tuning X-Planner, we integrate datasets used in the original GLaMM [32] model, including Semantic_Segm, RefCoco_GCG, PSG_GCG, Flickr_GCG, and GranDf_GCG, in combination with our COMPIE training dataset including InstructPix2Pix_GCG, UltraEdit_GCG, SEEDX_GCG, and MULAN_GCG. The data source ratio for training our X-Planner is [1, 3, 3, 3, 1, 3, 3, 9, 9, 9]. 8 X-Plannerâ€™s Bounding Box Localization In Table 4, we present results on the MULAN validation benchmark [41], demonstrating the localization effectiveness of our X-Planner. Key observations include: (1) X-Plannerâ€™s bounding box localization significantly improves with pseudo-labeling, where we annotate training data with bounding box generated by an MLLM pre-trained on insertion tasks (as described in section 3.2 â€“ Level 3 in the main paper). Mask Only baseline where we rely on segmentation decoder to predict mask of the location where object would be inserted is not sufficient. E.g., if we want to insert hat on person, then just having mask for the person is not sufficient as the hat would be beyond the person mask, hence we need to take the advantage of our MLLM to predict the bounding box on top of the head. (2) Enlarging small bounding boxes (&lt;&lt;&lt; 5% of the image size) in the training set further enhances box prediction accuracy, addressing challenges posed by very small bounding boxes that make insertion tasks difficult for diffusion-based editing models. Finally, just predicting bounding box gives good localization capability but sometimes it may not cover full-extent of the object and miss some part which might need to be edited when the object is inserted. So, for better coverage of editing location, we also try combining both bounding box and mask which gives slightly better localization than just using bounding box. Also, as editing location guidance, most of the editing methods like UtlraEdit are more robust to bigger editing location compared to smaller one, hence adding mask along with bounding box is a better strategy as it will give larger coverage of the editing area. E.g. in the last row of Figure 6, for inserting robot on table, the mask gives the coverage of table where robot would be inserted and bounding box provides additional coverage on top of the table as the inserted robot would be sitting on the table. 9 Quantitative Comparison on Emu Edit We also compare the effectiveness of our X-Planner on Emu Edit test set [37] which is similar to MagicBrush [47] test set and focuses on simpler instructions. We apply our X-Planner with the UltraEdit [50] model and we can see in Table 5 that X-Planner improves the performance on all the identity preserving metrics (Lâ¢1ğ¿1L1italic_L 1, Câ¢Lâ¢Iâ¢Piâ¢mğ¶ğ¿ğ¼subscriptğ‘ƒğ‘–ğ‘šCLIP_{im}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_i italic_m end_POSTSUBSCRIPT, and Dâ¢Iâ¢Nâ¢Oğ·ğ¼ğ‘ğ‘‚DINOitalic_D italic_I italic_N italic_O) due to better instruction localization through the predicted mask and box which would be given as control guidance to UltraEdit. Also, the instruction following performance (measured by Câ¢Lâ¢Iâ¢Poâ¢uâ¢tğ¶ğ¿ğ¼subscriptğ‘ƒğ‘œğ‘¢ğ‘¡CLIP_{out}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT) is similar to the UltraEdit as most of the instructions are simple and do not require X-Planner to simplify it further. Apart from UltraEdit results, we also show results of other baseline editing methods like InstructPix2Pix [4], MagicBrush [47], EmuEdit [37], and OmniGen [45] at the top of Table 5 as reference. Note: Emu Edit [37] and UltraEdit [50] highlight that the MagicBrush benchmark introduces biases favoring models trained on its dataset, leading to inflated performance, as the numbers highlighted in red for Câ¢Lâ¢Iâ¢Piâ¢mğ¶ğ¿ğ¼subscriptğ‘ƒğ‘–ğ‘šCLIP_{im}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_i italic_m end_POSTSUBSCRIPT and Dâ¢Iâ¢Nâ¢Oğ·ğ¼ğ‘ğ‘‚DINOitalic_D italic_I italic_N italic_O metrics in Table 5. This overfitting undermines the general editing capabilities of these models on other datasets. 10 Multi-Step Editing Error Propagation Our X-Planner is less prone to errors since it decomposes complex instructions into simpler, model-friendly steps. As stated in the main paper, we can further enhance reliability by introducing a closed-loop verification mechanism using strong MLLMs (e.g., InternVL2.5-38B [8] and GPT-4o [1]) to evaluate each intermediate result. Specifically, after each editing step, we use the verifier to assign a score from 0â€“4 that reflects how well the generated image aligns with the current instructionâ€”similar in spirit to the Mâ¢Lâ¢Lâ¢Mtâ¢iğ‘€ğ¿ğ¿subscriptğ‘€ğ‘¡ğ‘–MLLM_{ti}italic_M italic_L italic_L italic_M start_POSTSUBSCRIPT italic_t italic_i end_POSTSUBSCRIPT metric introduced in our evaluation. If the score falls below a threshold (e.g., 3), we automatically re-generate the step using a different random seed to recover from potential hallucinations, misalignment. This mechanism is critical for catching early-stage failures that would otherwise propagate through subsequent steps. We allow a configurable number of retries (e.g., max=1 or 4), striking a balance between quality and efficiency. As shown in Table 6, this approach improves instruction-image alignment and identity preservation compared to baselines without verification. Figure 11: X-Plannerâ€™s Training Dataset Summary (Generated from Pixtral-Large). This figure illustrates key statistics of our automatically constructed dataset used to train X-Planner using an open-sourced model, comprising around 300K instruction-image pairs. (a) Source Composition: The data is aggregated from four datasetsâ€“SEEDX (32.9%), UltraEdit (28.1%), MULAN (25.5%), and InstructPix2Pix (13.5%)â€”with nğ‘›nitalic_n indicating sample count. (b) Edit Type Distribution: Our dataset covers diverse editing intents, including insertion (35.0%), replace (16.2%), style (11.9%), background edits (10.1%), local texture (9.6%), local color change (9.7%), shape change (5.9%), and remove (1.6%). This diverse mix supports robust generalization across edit semantics. (c) Instruction Decomposition Complexity: While the majority (68.0%) of prompts require only a single edit, a substantial portion involve multi-step reasoning: 2-step (8.9%), 3-step (13.4%), 4-step (9.6%), and even 5-step (0.03%). This highlights the need for a sequential planner like X-Planner to handle compositional and complex instructions effectively. 11 Generate Training Data from Open-Sourced Model, Pixtral-Large To ensure reproducibility and accessibility of our pipeline, we also build a secondary version of the training dataset using an open-sourced MLLM, Pixtral-Large [2], a 124B-parameter multimodal model built upon Mistral Large v2, offering competitive performance on a range of VQA and multimodal reasoning benchmarks. Notably, it surpasses the closed-source GPT-4o [1] and Gemini 1.5 [39] models on several standard evaluation sets, making it a strong candidate for high-quality instruction generation while remaining fully accessible to the research community. We deploy Pixtral-Large [2] using 8 NVIDIA A100 80GB GPUs to enable inference across diverse image-to-instruction generation tasks. The instruction generation process mirrors the methodology used with GPT-4o to ensure a fair comparison: As shown in Figure 11, we use the same image datasets, including InstructPix2Pix [4], UltraEdit-100K [50], and SEEDX [12]. The dataset includes around 300K image and instruction pairs. For SEEDX, we generate both complex and simplified instruction pairs from the same image while for InstructPix2Pix, only complex instructions are generated, as simplified ones are already annotated. 12 Train X-Planner with Generated Data from Open-Sourced Model, Pixtral-Large To ensure a fair comparison, we maintain identical training settings, including model architecture, learning rate, batch size, and number of epochs, as described in Section 7 which were used for GPT-4o version of the X-Planner. This allows us to isolate the effect of training signal quality from the underlying data generator. We evaluate the Pixtral-Large-trained X-Planner across two benchmarks: 1) MagicBrush test set, and 2) COMPIE benchmark to compare with the GPT-4o version of the X-Planner. Table 7(b) (MagicBrush Test Set): On both single-turn and multi-turn settings, the Pixtral-trained X-Planner sightly outperforms the GPT-4o-trained version. Notably, in the multi-turn evaluation with UltraEdit + Bag of Models, the the Pixtral-Large version of X-Planner achieves the highest across all settings. This suggests that Pixtral-generated instruction data have competitive performance in complex editing tasks over multiple rounds. Table 8 (COMPIE Benchmark): We can see X-Planner trained from Pixtral-Large [2] generated data achieves comparable or slightly improved performance compared to its GPT-4o counterpart across most metrics. For example, using X-Planner + UltraEdit combined with decomposed instructions with mask control, Pixtral-Large verson of X-Planner yields MLLMti = 0.7102 and MLLMim = 0.5765, which closely matches GPT-4oâ€™s 0.7061 and 0.5744, respectively. When paired with InstructPix2Pix*, the Pixtral-trained X-Planner achieves the best overall MLLMti and MLLMim score of 0.7431 and 0.6488, indicating stronger consistency in understanding editing outputs. These results demonstrate that training X-Planner using data generated from the open-sourced MLLM demonstrating our approach is generalizable and not restricted to the close-sourced GPT-4o. 13 Additional Qualitative Results In Figure 12, we provide a detailed look at X-Plannerâ€™s decomposition capabilities, emphasizing its ability to effectively manage a wide range of edit types by generating precise and context-aware segmentation masks. Each example demonstrates how X-Planner tailors its outputs to the specific requirements of the edit type. For instance, in Row 3, the [replace] edit features a sophisticated approach where both the â€beforeâ€ and â€afterâ€ masks are generated, combined into a single guidance mask. This unified mask, further enhanced with appropriate dilation, provides reliable boundaries for the editing model to ensure accurate representation and minimizing errors. These examples underscore X-Plannerâ€™s ability to deliver precise and adaptive control signals for complex image editing tasks. In Figure 13, we show (1) a comparative evaluation of X-Planner against several baseline methods for multi-turn editing on the MagicBrush dataset and (2) a qualitative comparison with the UltraEdit baseline on the COMPIE benchmark. In the context of multi-turn editing, X-Planner stands out by leveraging its generated masks to achieve superior identity preservation. In contrast, many baseline methods, particularly InstructPix2Pix, often over-edit the image, highlighting their lack of fine-grained control and precision. These results underscore X-Plannerâ€™s ability to maintain consistency and accuracy across iterative edits. In Figure 14 and Figure 15, we present a comparison between InstructPix2Pix* results with and without the integration of X-Planner. For the [replace] edit type, as shown in Rows 4 in Figure 15, X-Planner produces a carefully dilated segmentation mask for the replaced region, enabling the editing model to execute the changes more effectively. Likewise, in Row 2, the [shape change] edit incorporates a dilated mask, accommodating potential alterations in the objectâ€™s shape and ensuring precise adjustments. These examples demonstrate the distinct advantages of X-Plannerâ€™s instruction decomposition and mask generation capabilities, providing enhanced control and accuracy compared to the baseline, which relies on directly processing complex instructions without decomposition. 14 Non-Rigid Edits on X-Planner In Figure 16, we present a diverse set of non-rigid and compositional edits using three different editing models: InstructPix2Pix*, GPT-4o [1], and IC-Edit [49]. These examples demonstrate the plug-and-play flexibility of our X-Planner, which can integrate with existing editing models and enable them to perform complex editsâ€”without requiring training on such complex instructionsâ€”by decomposing them into simpler, model-friendly steps. However, as our framework relies on external editing models for actual image generation, the final results are sometimes bounded by their limitations. For example, GPT-4o often fails to preserve fine identity details across steps, particularly in face edits (e.g., Row 4 in Figure 16, see red boxes). Similarly, UltraEdit [50] occasionally generates inserted objects that slightly exceed their designated bounding box regions (see Main Paper, Figure 6), especially in cat in AI world scenes. These limitations are inherent to the editing models instead of the planning framework. Table 4: Bounding Box Localization on MULAN Benchmark. X-Planner achieves strong localization gains by combining masks with MLLM-predicted boxes. Pseudo-labeling improves AP50 by over 2Ã—\\timesÃ— at K=1, and box enlargement further boosts recall at K=5. Best performance is achieved using both mask and box cues. Insertion Edit Task (n = 416) Method Setting K=1 K=3 K=5 IoUâ†‘â†‘\\uparrowâ†‘ AP50â†‘â†‘\\uparrowâ†‘ IoUâ†‘â†‘\\uparrowâ†‘ AP50â†‘â†‘\\uparrowâ†‘ IoUâ†‘â†‘\\uparrowâ†‘ AP50â†‘â†‘\\uparrowâ†‘ X-Planner Mask Only 0.37 0.34 0.46 0.37 0.54 0.38 +Pseudo-Label 0.63 0.70 0.71 0.69 0.75 0.69 + Box Enlarge 0.75 0.77 0.81 0.82 0.86 0.84 + Mask &amp; Box 0.73 0.78 0.81 0.86 0.85 0.86 Table 5: Quantitative Comparison on the Emu Edit Test. X-Planner significantly enhances UltraEditâ€™s editing quality by decomposing complex instructions and supplying additional control inputs. We report metrics including L1 distance (lower is better), CLIPimim{}_{\\text{im}}start_FLOATSUBSCRIPT im end_FLOATSUBSCRIPT and CLIPoutout{}_{\\text{out}}start_FLOATSUBSCRIPT out end_FLOATSUBSCRIPT similarity (higher is better), and DINO feature similarity. Compared to baseline methods such as InstructPix2Pix, MagicBrush, EmuEdit, and UltraEdit, X-Planner variants achieve consistently better performance across all metrics. Notably, the best results are obtained when combining X-Plannerâ€™s mask and bounding box with a diverse bag of editing models. Methods Guidance Control Input L1â†“â†“\\downarrowâ†“ CLIPim â†‘â†‘\\uparrowâ†‘ CLIPoutâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ InstructPix2Pix(450K) No 0.1213 0.8518 0.2742 0.7656 MagicBrush(450+20K) No 0.0652 0.9179 0.2763 0.8964 EmuEdit(10M) No 0.0895 0.8622 0.2843 0.8358 OmniGen No - 0.8360 0.2330 0.8040 UltraEdit (1M w/o region data) No 0.0515 0.8915 0.2804 0.8656 UltraEdit (3M w/o region data) No 0.0713 0.8446 0.2832 0.7937 UltraEdit No 0.0611 0.8627 0.2802 0.8079 X-Planner + UltraEdit X-Plannerâ€™s Mask 0.0462 0.9007 0.2782 0.8723 X-Planner + UltraEdit X-Plannerâ€™s Seg. Mask + X-Plannerâ€™s Bounding Box 0.0457 0.9029 0.2798 0.8766 X-Planner + Bag of Models X-Plannerâ€™s Seg. Mask + X-Plannerâ€™s Bounding Box 0.0443 0.9046 0.2822 0.8754 Table 6: Effectiveness of MLLM-Based Verification and Correction on COMPIE Evaluation. We evaluate X-Planner with closed-loop verification using GPT-4o and InternVL2.5-38B as step-wise verifiers. Our method achieves consistent gains over baselines by correcting intermediate errors through re-generation. We report improvements in both instruction-image alignment (Mâ¢Lâ¢Lâ¢Mtâ¢iğ‘€ğ¿ğ¿subscriptğ‘€ğ‘¡ğ‘–MLLM_{ti}italic_M italic_L italic_L italic_M start_POSTSUBSCRIPT italic_t italic_i end_POSTSUBSCRIPT) and visual consistency (Mâ¢Lâ¢Lâ¢Miâ¢mğ‘€ğ¿ğ¿subscriptğ‘€ğ‘–ğ‘šMLLM_{im}italic_M italic_L italic_L italic_M start_POSTSUBSCRIPT italic_i italic_m end_POSTSUBSCRIPT), highlighting the benefits of MLLM-guided correction for complex multi-step editing. Methods Guidance &amp; Error Verify L1â†“â†“\\downarrowâ†“ CLIPim â†‘â†‘\\uparrowâ†‘ CLIPoutâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ MLLMti â†‘â†‘\\uparrowâ†‘ MLLMim â†‘â†‘\\uparrowâ†‘ SmartEdit No 0.2764 0.7713 0.2512 0.6044 0.6511 0.5347 MGIE No 0.2988 0.7692 0.2498 0.5981 0.6408 0.5288 UltraEdit (UE) No 0.1292 0.7688 0.2698 0.6387 0.6652 0.5523 X-Planner+UE Decomp.+No Verification 0.1253 0.7767 0.2621 0.6435 0.6894 0.5593 X-Planner+UE Mask + Decomp.+No Verification 0.1188 0.7875 0.2569 0.6599 0.7061 0.5744 X-Planner+UE (GPT-4o) Mask+Decomp.+Verification (max: 1) 0.1175 0.7853 0.2563 0.6612 0.7113 0.5798 X-Planner+UE (GPT-4o) Mask+Decomp.+Verification (max: 4) 0.1163 0.7942 0.2574 0.6673 0.7308 0.5936 X-Planner+UE(InternVL) Mask+Decomp.+Verification (max: 1) 0.1180 0.7861 0.2559 0.6632 0.7128 0.5822 X-Planner+UE(InternVL) Mask+Decomp.+Verification (max: 4) 0.1160 0.7901 0.2571 0.6647 0.7258 0.5955 Table 7: Quantitative Comparison on the MagicBrush Test Set (X-Planner Trained from GPT-4o vs Pixtral-Large Generated Data). We show single-turn (left) and multi-turn (right) performance. Our approach, using predicted masks and boxes, rivals or outperforms UltraEdit with human labels. â€Bag of Modelsâ€ uses PowerPaint [36] for removal, InstructDiff [13] for style, and UltraEdit otherwise. (a) Single-Turn Editing Methods Control L1â†“â†“\\downarrowâ†“ L2â†“â†“\\downarrowâ†“ CLIP-Iâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ UltraEdit (UE) No 0.0614 0.0181 0.9197 0.8804 UltraEdit (UE) Human Mask 0.0575 0.0172 0.9307 0.8982 X-Planner (GPT-4o Generated Training Data) X-Planner+UE Mask 0.0528 0.0171 0.9281 0.8900 X-Planner+UE Mask+Box 0.0513 0.0168 0.9312 0.8959 X-Planner+Bag of Models Mask+Box 0.0511 0.0172 0.9331 0.8970 X-Planner (Pixtral Generated Training Data) X-Planner+UE Mask 0.0529 0.0173 0.9300 0.8908 X-Planner+UE Mask+Box 0.0508 0.0165 0.9324 0.8983 X-Planner+Bag of Models Mask+Box 0.0509 0.0174 0.9342 0.8985 (b) Multi-Turn Editing Methods Control L1â†“â†“\\downarrowâ†“ L2â†“â†“\\downarrowâ†“ CLIP-Iâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ UE w/o region No 0.0780 0.0246 0.8954 0.8322 UE w/ region Human Mask 0.0745 0.0236 0.9045 0.8505 X-Planner (GPT-4o Generated Training Data) X-Planner+UE Mask 0.0679 0.0227 0.9025 0.8423 X-Planner+UE Mask+Box 0.0668 0.0226 0.9047 0.8475 X-Planner+Bag of Models Mask+Box 0.0665 0.0223 0.9079 0.8508 X-Planner (Pixtral Generated Training Data) X-Planner+UE Mask 0.0685 0.0230 0.9031 0.8429 X-Planner+UE Mask+Box 0.0669 0.0225 0.9057 0.8471 X-Planner+Bag of Models Mask+Box 0.0661 0.0222 0.9083 0.8514 Table 8: Quantitative Comparison on the COMPIE Benchmark (X-Planner Trained from GPT4o vs Pixtral-Large Generated Data). X-Planner significantly improves the editing performance of UltraEdit and InstructPix2Pix* by decomposing complex instructions and providing control guidance inputs (e.g., segmentation masks). To overcome the limitations of Câ¢Lâ¢Iâ¢Poâ¢uâ¢tğ¶ğ¿ğ¼subscriptğ‘ƒğ‘œğ‘¢ğ‘¡CLIP_{out}italic_C italic_L italic_I italic_P start_POSTSUBSCRIPT italic_o italic_u italic_t end_POSTSUBSCRIPT in handling complex instructions, we utilize an MLLM as an alternative evaluation metric to highlight capabilities of X-Planner. Methods Guidance Control Input L1â†“â†“\\downarrowâ†“ CLIPim â†‘â†‘\\uparrowâ†‘ CLIPoutâ†‘â†‘\\uparrowâ†‘ DINOâ†‘â†‘\\uparrowâ†‘ MLLMti â†‘â†‘\\uparrowâ†‘ MLLMim â†‘â†‘\\uparrowâ†‘ SmartEdit No 0.2764 0.7713 0.2512 0.6044 0.6511 0.5347 MGIE No 0.2988 0.7692 0.2498 0.5981 0.6408 0.5288 UltraEdit No 0.1292 0.7688 0.2698 0.6387 0.6652 0.5523 X-Planner Trained from GPT4o Generated Data X-Planner + UltraEdit X-Plannerâ€™s Decomposed Instruction 0.1253 0.7767 0.2621 0.6435 0.6894 0.5593 X-Planner + UltraEdit X-Plannerâ€™s Mask + Decomposed Instruction 0.1188 0.7875 0.2569 0.6599 0.7061 0.5744 X-Planner Trained from Pixtral-Large Generated Data X-Planner + UltraEdit X-Plannerâ€™s Decomposed Instruction 0.1261 0.7744 0.2630 0.6428 0.6904 0.5626 X-Planner + UltraEdit X-Plannerâ€™s Mask + Decomposed Instruction 0.1207 0.7853 0.2584 0.6577 0.7102 0.5765 InstructPix2Pix* No 0.1517 0.8020 0.2666 0.6988 0.6727 0.6160 X-Planner Trained from GPT4o Generated Data X-Planner + InstructPix2Pix* X-Plannerâ€™s Decomposed Instruction 0.1458 0.8143 0.2641 0.7114 0.7072 0.6277 X-Planner + InstructPix2Pix* X-Plannerâ€™s Mask + Decomposed Instruction 0.1320 0.8285 0.2591 0.7068 0.7408 0.6454 X-Planner Trained from Pixtral-Large Generated Data X-Planner + InstructPix2Pix* X-Plannerâ€™s Decomposed Instruction 0.1460 0.8141 0.2655 0.7122 0.7088 0.6295 X-Planner + InstructPix2Pix* X-Plannerâ€™s Mask + Decomposed Instruction 0.1325 0.8291 0.2586 0.7077 0.7431 0.6488 Figure 12: X-Plannerâ€™s Decomposition Outputs (Simplified Instructions, Edit Types, Masks, and Bounding Boxes). This figure illustrates examples of X-Plannerâ€™s decomposition results, showcasing its ability to handle diverse edit types. The segmentation masks generated are tailored to the specific edit type. For instance, in Row 3, the [replace] edit features both before and after masks which are combined into a single mask for editing guidance, with appropriate dilation applied to ensure accurate representation. Also, in Row 1, the [shape change] edit provides more dilated mask by giving more rooms for potential shape modification. Figure 13: Qualitative Comparison on MagicBrush, and on COMPIE Benchmark between X-Planner and UltraEdit (I). This figure presents: (1) a comparison of X-Planner with several baseline methods for multi-turn editing on the MagicBrush dataset and (2) a qualitative comparison with the UltraEdit baseline on the COMPIE benchmark. For multi-turn editing, X-Planner, guided by its generated masks, demonstrates superior identity preservation. In contrast, many baseline methods, especially InstructPix2Pix, tend to overedit the image due to a lack of control. Figure 14: Qualitative Comparison between X-Planner and both InstructPix2Pix* (I) and UltraEdit Baseline (II). This figure compares results from both InstructPix2Pix* and UltraEdit Baseline with and without the integration of X-Planner. These examples highlight the advantages of X-Plannerâ€™s instruction decomposition and mask controllability over the baseline which is directly given with complex instruction. Figure 15: Qualitative Comparison between X-Planner and InstructPix2Pix* Baseline (II). This figure compares results from InstructPix2Pix* with and without the integration of X-Planner. For the [replace] edit type in Rows 4, X-Planner generates a dilated segmentation mask for the replaced region to better accommodate the editing model. Similarly, in Row 2, the [shape change] edit includes a dilated mask to account for potential changes. These examples highlight the advantages of X-Plannerâ€™s instruction decomposition and mask controllability over the baseline which is directly given with complex instruction. Figure 16: Examples of Non-Rigid and Compositional Edits with Model-Specific Failure Cases. We show diverse complex edits (especially the [shape change] edits) generated by plugging X-Planner into three editing models: InstructPix2Pix*, GPT-4o [1], and IC-Edit [49]. Despite not being trained on such complex instructions, these models can perform challenging edits thanks to our decomposition and localization planning. However, some failure cases arise from the editing models: GPT-4o struggles with identity preservation in face edits (e.g., Row 4, red boxes).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "def fetch_arxiv_article_main_text(html_link):\n",
    "    try:\n",
    "        resp = requests.get(html_link, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        html = resp.text\n",
    "        # æå– <article ...> ... </article> ä¹‹é—´å†…å®¹\n",
    "        match = re.search(r\"<article.*?>(.*?)</article>\", html, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            article_html = match.group(1)\n",
    "            # å»é™¤æ‰€æœ‰ HTML æ ‡ç­¾\n",
    "            text = re.sub(r\"<.*?>\", \"\", article_html)\n",
    "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        else:\n",
    "            text = \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "main_text = fetch_arxiv_article_main_text(\"http://arxiv.org/html/2507.05259v1\")\n",
    "print(main_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
